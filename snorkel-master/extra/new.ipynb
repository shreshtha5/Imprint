{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import TextDocPreprocessor2\n",
    "doc_preprocessor = TextDocPreprocessor2('data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========                                ] 20%data2/doc7\n",
      "[========                                ] 20%data2/doc40\n",
      "[================                        ] 40%data2/doc116\n",
      "[========================                ] 60%data2/doc137\n",
      "[================================        ] 80%data2/doc19\n",
      "[========================================] 100%data2/doc20\n",
      "[================================================] 120%data2/doc71\n",
      "data2/doc14\n",
      "data2/doc95\n",
      "data2/doc59\n",
      "data2/doc58\n",
      "data2/doc3\n",
      "data2/doc141\n",
      "data2/doc56\n",
      "data2/doc104\n",
      "data2/doc13\n",
      "data2/doc72\n",
      "data2/doc129\n",
      "data2/doc9\n",
      "data2/doc78\n",
      "data2/doc106\n",
      "data2/doc93\n",
      "data2/doc131\n",
      "data2/doc51\n",
      "data2/doc8\n",
      "data2/doc94\n",
      "data2/doc47\n",
      "data2/doc23\n",
      "data2/doc144\n",
      "data2/doc67\n",
      "data2/doc74\n",
      "data2/doc44\n",
      "data2/doc70\n",
      "data2/doc64\n",
      "data2/doc133\n",
      "data2/doc124\n",
      "data2/doc118\n",
      "data2/doc111\n",
      "data2/doc100\n",
      "data2/doc82\n",
      "data2/doc114\n",
      "data2/doc123\n",
      "data2/doc69\n",
      "data2/doc4\n",
      "data2/doc103\n",
      "data2/doc12\n",
      "data2/doc80\n",
      "data2/doc149\n",
      "data2/doc21\n",
      "data2/doc61\n",
      "data2/doc79\n",
      "data2/doc24\n",
      "data2/doc34\n",
      "data2/doc36\n",
      "data2/doc85\n",
      "data2/doc54\n",
      "data2/doc128\n",
      "data2/doc132\n",
      "data2/doc105\n",
      "data2/doc53\n",
      "data2/doc92\n",
      "data2/doc49\n",
      "data2/doc112\n",
      "data2/doc102\n",
      "data2/doc109\n",
      "data2/doc28\n",
      "data2/doc142\n",
      "data2/doc76\n",
      "data2/doc87\n",
      "data2/doc77\n",
      "data2/doc17\n",
      "data2/doc2\n",
      "data2/doc88\n",
      "data2/doc33\n",
      "data2/doc25\n",
      "data2/doc32\n",
      "data2/doc90\n",
      "data2/doc110\n",
      "data2/doc5\n",
      "data2/doc99\n",
      "data2/doc31\n",
      "data2/doc122\n",
      "data2/doc29\n",
      "data2/doc101\n",
      "data2/doc63\n",
      "data2/doc48\n",
      "data2/doc10\n",
      "data2/doc148\n",
      "data2/doc138\n",
      "data2/doc120\n",
      "data2/doc35\n",
      "data2/doc75\n",
      "data2/doc108\n",
      "data2/doc62\n",
      "data2/doc11\n",
      "data2/doc16\n",
      "data2/doc73\n",
      "data2/doc15\n",
      "data2/doc134\n",
      "data2/doc52\n",
      "data2/doc97\n",
      "data2/doc89\n",
      "data2/doc26\n",
      "data2/doc60\n",
      "data2/doc140\n",
      "data2/doc119\n",
      "data2/doc6\n",
      "data2/doc42\n",
      "data2/doc38\n",
      "data2/doc50\n",
      "data2/doc30\n",
      "data2/doc81\n",
      "data2/doc22\n",
      "data2/doc1\n",
      "data2/doc39\n",
      "data2/doc127\n",
      "data2/doc151\n",
      "data2/doc65\n",
      "data2/doc68\n",
      "data2/doc126\n",
      "data2/doc139\n",
      "data2/doc66\n",
      "data2/doc55\n",
      "data2/doc147\n",
      "data2/doc96\n",
      "data2/doc86\n",
      "data2/doc37\n",
      "data2/doc41\n",
      "data2/doc27\n",
      "data2/doc107\n",
      "data2/doc125\n",
      "data2/doc121\n",
      "data2/doc150\n",
      "data2/doc18\n",
      "data2/doc98\n",
      "data2/doc83\n",
      "data2/doc145\n",
      "data2/doc117\n",
      "data2/doc46\n",
      "data2/doc45\n",
      "data2/doc84\n",
      "data2/doc135\n",
      "data2/doc136\n",
      "data2/doc115\n",
      "data2/doc130\n",
      "data2/doc43\n",
      "data2/doc113\n",
      "data2/doc146\n",
      "data2/doc143\n",
      "data2/doc91\n",
      "data2/doc57\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 5.87 s, sys: 124 ms, total: 5.99 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "\n",
    "n_docs=5\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 151)\n",
      "('Sentences:', 1609)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Disaster = candidate_subclass('Disaster', ['Name','Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DateMatcher, LocationMatcher\n",
    "ngrams         = Ngrams(n_max=7)\n",
    "#person_matcher = PersonMatcher(longest_match_only=True)\n",
    "location_matcher = LocationMatcher(longest_match_only=True)\n",
    "date_matcher=DateMatcher(longest_match_only=True)\n",
    "#dict=['earthquake','flood','cyclone','fire']\n",
    "#dictionary_matcher=DictionaryMatch(d=dict)\n",
    "cand_extractor = CandidateExtractor(Disaster, [ngrams, ngrams], [date_matcher, location_matcher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "#from util import number_of_people\n",
    "\n",
    "\n",
    "docs = session.query(Document)\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 3 == 2:\n",
    "            dev_sents.add(s)\n",
    "        elif i % 3 == 1:\n",
    "            dev_sents.add(s)\n",
    "        else:\n",
    "            dev_sents.add(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1609, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_sents),len(train_sents),len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(Disaster).filter(Disaster.split == i).count())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "                   \n",
    "meteorological_disasters = { 'storm', 'bizzard', 'tornado', 'hurricane', 'cyclone', 'avalanches', 'heatwave','coldwave' 'drought', 'hailstorm'}\n",
    "hydrological_disasters = { 'flood' , 'tsunami', 'limnic eruption' , 'famine' , 'forest fire' }\n",
    "\n",
    "geological_disasters = { 'earthquake', 'volcano' , 'rock fall'}\n",
    "\n",
    "                   \n",
    "biological_disasters = { 'epidemic', 'pandemic' , 'influenza'}\n",
    "\n",
    "manmade_disasters = { 'blast', 'bomb explosion', 'bomb blast', 'bomb', 'bombings', 'surgical strike', 'shooting', 'firing', 'gun fire' , 'riots' , 'armed conflict'}\n",
    "\n",
    "def LF_mdisaster(c):\n",
    "     return 1 if len(meteorological_disasters.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_hdisaster(c):\n",
    "    return 1 if len(hydrological_disasters.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_gdisaster(c):\n",
    "    return 1 if len(geological_disasters.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_bdisaster(c):\n",
    "    return 1 if len(biological_disasters.intersection(get_between_tokens(c))) > 0 else 0\n",
    "\n",
    "def LF_mandisaster(c):\n",
    "    return 1 if len(manmade_disasters.intersection(get_between_tokens(c))) > 0 else 0\n",
    "def LF_tdisaster(c):\n",
    "      return 1 if 'train’' in get_between_tokens(c) and 'collided’' in get_right_tokens(c) else 0\n",
    "def LF_killed(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+killed+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I)  else 0\n",
    "\n",
    "def LF_injured(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+injured+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I)  else 0\n",
    "\n",
    "def LF_dead(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+dead+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_damage(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+heavy+\\s+damage+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_conflict(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+conflict+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "trigger1 = ['killed', 'kills', 'killing']\n",
    "\n",
    "def LF_kill(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger1) + '.\\w', 1)\n",
    "trigger2 = ['violence', 'violent', 'rioting']\n",
    "def LF_violence(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger2) + '.\\w', 1)\n",
    "trigger3 = ['crashed’,’collided’,’hit’']\n",
    "\n",
    "def LF_crash(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger3) + '.\\w', 1)\n",
    "\n",
    "trigger4 = ['surgical strike', 'strike', 'struck', 'hit the target']\n",
    "\n",
    "def LF_strike(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger4) + '.\\w', 1)\n",
    "\n",
    "trigger5 = ['cold', 'heat']\n",
    "\n",
    "def LF_wave(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger4) + \"wave\" + '.\\w', 1)\n",
    "def LF_cold(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+cold+arctic +air+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "trigger6 = ['flood' ,'floods' , 'flooding', 'flooded']\n",
    "def LF_flood(c):\n",
    "     return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger4) + '.\\w', 1)\n",
    "\n",
    "trigger7 = ['eruption' ,'erupting' , 'erupted',  'erupt']\n",
    "\n",
    "def LF_eruption(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger7) + '.\\w', 1)\n",
    "\n",
    "trigger8 = ['fell off' ,'slide', 'fall']\n",
    "\n",
    "#def LF_rock(c):\n",
    "    #return rule_regex_search_btw_AB(c, '.\\w' + rock + ltp(trigger8) + '.\\w', 1)\n",
    "\n",
    "trigger9 = ['cholera' ,'ebola', 'dengue']\n",
    "\n",
    "def LF_epidemic(c):\n",
    "    return rule_regex_search_btw_AB(c, '.\\w' + ltp(trigger9) + '.\\w', 1)\n",
    "\n",
    "\n",
    "def LF_tsunami(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+tsunami+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_confront(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+armed+confrontation+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_rammed(c):\n",
    "    return 1 if re.search (r'{{A}}+LF_rammed\\w+\\s+rammed+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_smashed(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+smashed+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_exploded(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+exploded+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_shipwrecked(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+shipwrecked+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_nuclearmeltdown(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+nuclear meltdown.+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_tornado(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+tornado(es)+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_limniceruption(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+limnic eruption+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0\n",
    "\n",
    "def LF_wildfire(c):\n",
    "    return 1 if re.search (r'{{A}}+\\w+\\s+wildfire(es)+\\s+\\w+{{B}}',  get_tagged_text(c), flags =re.I) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = []\n",
    "for c in session.query(Disaster).filter(Disaster.split == 1).all():\n",
    "    if LF_mandisaster(c) != 0:\n",
    "        labeled.append(c)\n",
    "print(\"Number labeled:\", len(labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "     LF_mdisaster,  LF_gdisaster,  LF_hdisaster,  LF_bdisaster,  LF_mandisaster, LF_tdisaster, LF_wildfire,LF_limniceruption, LF_tornado\n",
    ",LF_nuclearmeltdown, LF_shipwrecked  ,LF_exploded, LF_smashed, LF_rammed,  LF_confront,LF_tsunami ,LF_epidemic,LF_eruption,LF_flood,\n",
    "LF_wave,LF_strike, LF_crash, LF_violence, LF_kill, LF_conflict, LF_damage, LF_dead, LF_injured, LF_kill ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<0x0 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "#import sys\n",
    "#stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n",
    "#reload(sys)\n",
    "#sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n",
    "\n",
    "\n",
    "\n",
    "#np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=1)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<0x0 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = labeler.load_matrix(session, split=1)\n",
    "L_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-01547c031c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shreshtha/2nd_sem/seminar/snorkel-master/snorkel/annotations.pyc\u001b[0m in \u001b[0;36mget_candidate\u001b[0;34m(self, session, i)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m\"\"\"Return the Candidate object corresponding to row i\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_row_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "L_train.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.get_key(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [j, Coverage, Overlaps, Conflicts]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_marginals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-81720f98b37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_marginals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_marginals' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5): \n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "x=predict_labels(train_marginals)\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779\n",
      "['-1' '-1' '1' '1' '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1' '1' '-1'\n",
      " '-1' '1' '-1' '1' '-1' '-1' '-1' '1' '-1\\n' '-1' '1' '-1' '-1' '1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '1'\n",
      " '-1' '-1' '-1' '-1' '-1' '-1' '1' '-1\\n' '-1' '-1' '-1' '-1' '-1' '-1'\n",
      " '-1' '1' '-1\\n' '-1' '1' '-1' '-1' '1' '1' '-1' '1' '-1' '-1' '-1' '1'\n",
      " '-1\\n' '-1' '1' '1' '-1' '1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1'\n",
      " '-1' '1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1\\n' '-1' '1' '-1' '1' '-1' '1'\n",
      " '-1' '-1' '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '1' '1' '-1' '1'\n",
      " '-1' '1' '-1' '-1' '-1' '-1\\n' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1\\n'\n",
      " '-1' '-1' '1' '-1' '-1' '1' '-1' '1' '-1' '1' '1' '1' '1' '-1\\n' '-1'\n",
      " '-1\\n' '-1\\n' '-1\\n' '-1' '1' '1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1'\n",
      " '1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1\\n' '-1' '-1' '-1' '1'\n",
      " '-1\\n' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '1' '-1' '-1' '1' '-1' '-1'\n",
      " '-1' '-1' '-1' '1' '-1' '-1\\n' '-1' '1' '-1' '-1' '-1' '-1' '-1\\n' '-1'\n",
      " '-1' '1' '-1' '-1' '-1' '-1' '1' '-1\\n' '-1' '-1' '-1' '-1\\n' '-1' '-1\\n'\n",
      " '-1\\n' '-1' '-1' '1' '-1' '-1' '-1' '1' '1' '1' '-1' '1' '1' '-1' '1' '-1'\n",
      " '1' '-1' '-1' '1' '1' '-1\\n' '-1' '-1' '1' '-1' '-1' '1' '-1' '1' '1' '-1'\n",
      " '-1' '1' '1' '-1' '-1' '1' '1' '-1' '-1' '-1' '-1' '1' '-1\\n' '-1' '-1'\n",
      " '-1' '-1\\n' '1' '-1' '1' '-1' '1' '-1' '-1' '1' '1' '-1\\n' '-1' '1' '1'\n",
      " '-1' '-1' '-1' '1' '-1' '-1\\n' '-1' '1' '-1' '-1' '-1' '1' '-1' '-1' '1'\n",
      " '-1' '1' '-1' '1' '-1' '-1' '1' '-1' '-1' '-1' '1' '-1' '-1' '1' '1' '-1'\n",
      " '-1' '1' '1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '1' '-1\\n' '-1' '-1\\n' '-1'\n",
      " '-1' '1' '-1\\n' '-1\\n' '-1' '-1' '1' '-1\\n' '-1' '1' '-1' '-1' '-1' '-1'\n",
      " '-1' '1' '1' '-1\\n' '1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '1' '-1' '-1'\n",
      " '1' '1' '-1' '-1\\n' '-1' '-1' '-1' '-1\\n' '-1' '-1' '-1' '1' '-1' '-1'\n",
      " '-1' '-1' '-1' '1' '-1' '-1' '-1' '1' '1' '-1\\n' '-1' '-1' '1' '1' '-1'\n",
      " '1' '1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1' '-1\\n'\n",
      " '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '1' '-1\\n' '-1'\n",
      " '-1' '-1' '1' '-1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1' '-1' '1' '-1' '-1'\n",
      " '1' '-1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1\\n' '-1' '-1' '-1' '1' '-1'\n",
      " '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1\\n' '-1' '1' '-1\\n' '-1' '-1' '-1'\n",
      " '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1'\n",
      " '-1' '1' '-1\\n' '-1' '1' '-1' '-1' '1' '-1' '1' '-1' '1' '1' '-1' '1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1\\n' '1' '-1' '-1' '-1' '-1\\n' '1' '-1' '1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '1' '-1' '-1\\n' '-1' '-1' '-1' '1'\n",
      " '-1' '-1' '-1' '1' '-1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1' '-1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1\\n' '-1' '1' '-1' '1' '-1' '1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '1' '1' '-1' '-1\\n' '-1'\n",
      " '-1' '1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '1' '-1' '-1' '1' '-1' '-1'\n",
      " '-1' '-1' '-1' '1' '1' '-1' '-1\\n' '-1' '-1' '-1' '-1' '-1' '-1' '-1\\n'\n",
      " '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1\\n' '-1' '-1' '-1' '-1' '1' '-1' '1'\n",
      " '1' '-1' '1' '-1' '-1' '-1\\n' '1' '-1' '-1' '-1' '-1' '1' '1' '1' '1' '1'\n",
      " '-1\\n' '-1' '-1' '-1\\n' '1' '-1' '-1' '-1' '-1' '-1\\n' '-1\\n' '-1' '1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '1'\n",
      " '-1' '1' '1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1\\n' '-1' '-1'\n",
      " '-1' '1' '-1' '-1' '1' '-1' '-1' '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '1'\n",
      " '-1' '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1'\n",
      " '-1' '1' '-1' '1' '1' '1' '1' '1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1\\n'\n",
      " '-1' '-1' '-1' '-1' '-1' '-1' '-1\\n' '-1' '1' '1' '-1' '-1' '1' '-1' '-1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1\\n' '-1' '-1' '1' '1' '-1\\n' '-1' '-1' '1'\n",
      " '-1' '-1' '-1' '-1' '-1' '1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1' '-1'\n",
      " '-1' '-1']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "my_list=[]\n",
    "with open(\"/home/shreshtha/2nd_sem/seminar/snorkel-master/gold_labels.tsv\") as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    \n",
    "    my_list = [row[-1] for row in reader]\n",
    "\n",
    "for i in range(0,7):    \n",
    "    my_list.append('-1')\n",
    "print(len(my_list))\n",
    "list=np.array(my_list)\n",
    "print(list)\n",
    "#print(size(list))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c7073b9ba5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = labeler.apply_existing(split=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "from util import load_external_labels\n",
    "\n",
    "%time missed = load_external_labels(session, Disaster, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = labeler.apply_existing(split=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
